{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Task2_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "125cxV5M7BZGFJCePNWMhyThUXD2ZWTq-",
      "authorship_tag": "ABX9TyMQBGjXPC6dF+mNZLlvLW1d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prachi428/MIDAS_Task2_2021/blob/main/Task2_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tfj_EM7jiyx3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b279e37d-3cfe-4a7b-d6a3-5e4204896d02"
      },
      "source": [
        "%cd /content/drive/MyDrive/MCA\\ Project/MIDAS"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/MCA Project/MIDAS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlLQLSwdr1TI"
      },
      "source": [
        "import torch, glob, cv2, os, tqdm\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "import random\n",
        "from skimage import transform\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQNh4V5JCPO7"
      },
      "source": [
        "# files\n",
        "# %pwd\n",
        "image = cv2.imread(\"/content/drive/MyDrive/MCA Project/MIDAS/train/Sample001/img001-001.png\")\n",
        "image2 = cv2.imread(\"/content/drive/MyDrive/MCA Project/MIDAS/train/Sample001/img001-055.png\")\n",
        "image3 = cv2.imread(\"/content/drive/MyDrive/MCA Project/MIDAS/train/Sample061/img061-001.png\")\n",
        "test_set = []\n",
        "train_set = []\n",
        "for i in range(int(0.4*62)):\n",
        "  test_class = random.randint(1,62)\n",
        "  while(test_class in test_set):\n",
        "    test_class = random.randint(1, 62)\n",
        "  test_set.append(test_class)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4EtIqBSPa6L"
      },
      "source": [
        "len(test_set)\n",
        "for i in range(1, 63):\n",
        "  if(i not in test_set):\n",
        "    train_set.append(i)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tg6iukrLAHta"
      },
      "source": [
        "class GetData(Dataset):\n",
        "\n",
        "  def __init__(self, indices, dir=\"train/\", transform=None):\n",
        "    \n",
        "    self.transform = transform\n",
        "    self.files = {}\n",
        "    self.len = 0\n",
        "    self.ind = indices\n",
        "    for n in self.ind:\n",
        "      # print(n)\n",
        "      self.files[n] = []\n",
        "      n = str(n)\n",
        "      if(len(n) < 3):\n",
        "        for i in range(3 - len(n)):\n",
        "          n = '0' + n\n",
        "      for image in glob.glob(dir + \"Sample\" + n + '/*'):\n",
        "        self.files[int(n)].append(image) \n",
        "        self.len += 1  \n",
        "\n",
        "\n",
        "  \n",
        "  def __len__(self):\n",
        "    return self.len\n",
        "\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    idx_dict = int(idx/40)\n",
        "    idx_ls = idx - idx_dict*40\n",
        "    # print(idx_dict)\n",
        "    # print(idx_ls, len(self.files[idx_dict]))\n",
        "    img_name = self.files[self.ind[idx_dict]][idx_ls]\n",
        "    image = cv2.imread(img_name)\n",
        "\n",
        "\n",
        "    image_pos = random.randint(0, 39)\n",
        "    if(image_pos == idx_ls):\n",
        "      image_pos -= 1\n",
        "    if image_pos == -1:\n",
        "      image_pos = image_pos + 1\n",
        "    image_pos = cv2.imread(self.files[self.ind[idx_dict]][idx_ls])\n",
        "\n",
        "\n",
        "    idx_neg_dict = random.randint(0, len(self.ind)-1)\n",
        "    idx_neg = random.randint(0, 39)\n",
        "\n",
        "    if(idx_neg_dict == idx_dict):\n",
        "      idx_neg_dict -= 1\n",
        "    if idx_neg_dict == -1:\n",
        "      idx_neg_dict = idx_neg_dict+2\n",
        "\n",
        "\n",
        "    image_neg = cv2.imread(self.files[self.ind[idx_neg_dict]][idx_neg])\n",
        "\n",
        "\n",
        "    pair = {\"main\": image, \"pos\": image_pos, \"neg\": image_neg}\n",
        "    if self.transform:\n",
        "      pair = self.transform(pair)\n",
        "    return pair\n",
        "\n",
        "class ToTensor(object):\n",
        "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        img1, img2, img3 = sample[\"main\"], sample[\"pos\"], sample[\"neg\"]\n",
        "        # swap color axis because\n",
        "        # numpy image: H x W x C\n",
        "        # torch image: C X H X W\n",
        "        # print(img1.shape)\n",
        "\n",
        "        return {\"main\": torch.from_numpy(img1).type(torch.FloatTensor),\n",
        "        \"pos\": torch.from_numpy(img2).type(torch.FloatTensor),\n",
        "        \"neg\": torch.from_numpy(img3).type(torch.FloatTensor)}\n",
        "\n",
        "\n",
        "class Resize(object):\n",
        "    \"\"\"Rescale the image in a sample to a given size.\n",
        "\n",
        "    Args:\n",
        "        output_size (tuple or int): Desired output size. If tuple, output is\n",
        "            matched to output_size. If int, smaller of image edges is matched\n",
        "            to output_size keeping aspect ratio the same.\n",
        "    \"\"\"\n",
        "\n",
        "    # def __init__(self):\n",
        "    #     assert isinstance(output_size, (int, tuple))\n",
        "        # self.output_size = output_size\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        img1, img2, img3 = sample[\"main\"], sample[\"pos\"], sample[\"neg\"]\n",
        "        # swap color axis because\n",
        "        # numpy image: H x W x C\n",
        "        # torch image: C X H X W\n",
        "        img1 = transform.resize(img1[:, :, 0], (105, 105))\n",
        "        img2 = transform.resize(img2[:, :, 0], (105, 105))\n",
        "        img3 = transform.resize(img3[:, :, 0], (105, 105))\n",
        " \n",
        "        return {\"main\": img1,\n",
        "        \"pos\": img2,\n",
        "        \"neg\": img3}"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-zCdHs9O0fM"
      },
      "source": [
        "transformed_dataset = GetData(train_set, transform=transforms.Compose([Resize(), ToTensor()]))\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Syemy2UvQjGR",
        "outputId": "57891827-bd17-480c-f426-c33bee573d09"
      },
      "source": [
        "len_train = int(0.8*len(transformed_dataset))\n",
        "train, val = random_split(transformed_dataset, [len_train, int(len(transformed_dataset) - len_train)])\n",
        "len(transformed_dataset), len(train), len(val)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1520, 1216, 304)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGqqqeQnQnKX"
      },
      "source": [
        "train_loader = DataLoader(train, batch_size=40, shuffle=True)\n",
        "val_loader = DataLoader(val, batch_size=40, shuffle=True)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bT_CF1_xQ0wq"
      },
      "source": [
        "# for t in train_loader:\n",
        "#   print(t[\"main\"].shape)\n",
        "#   break"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgptL3KzQ3ZU"
      },
      "source": [
        "class SiameseNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(SiameseNetwork, self).__init__()\n",
        "\n",
        "    self.conv_model = nn.Sequential(\n",
        "      nn.Conv2d(in_channels=1, out_channels=96, kernel_size=10),\n",
        "      nn.MaxPool2d(2),\n",
        "      nn.ReLU(inplace=True),\n",
        "\n",
        "      \n",
        "      nn.Conv2d(in_channels=96, out_channels=128, kernel_size=7),\n",
        "      nn.MaxPool2d(2),\n",
        "      nn.ReLU(inplace=True),\n",
        "\n",
        "      nn.Conv2d(in_channels=128, out_channels=128, kernel_size=4),\n",
        "      nn.MaxPool2d(2),\n",
        "      nn.ReLU(inplace=True),\n",
        "      \n",
        "      nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4),\n",
        "      # nn.MaxPool2d(2),\n",
        "      nn.ReLU(inplace=True))\n",
        "    # for m in self.modules():\n",
        "    #         if isinstance(m, nn.Conv2d):\n",
        "    #             nn.init.kaiming_normal(m.weight, mode='fan_in')\n",
        "    \n",
        "    self.fc1 = nn.Linear(9216, 4096)\n",
        "    self.fc2 = nn.Linear(4096, 1)\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Conv2d):\n",
        "          nn.init.normal_(m.weight, 0, 1e-2)\n",
        "          nn.init.normal_(m.bias, 0.5, 1e-2)\n",
        "      elif isinstance(m, nn.Linear):\n",
        "          nn.init.normal_(m.weight, 0, 2e-1)\n",
        "          # nn.init.normal(m.weight, 0, 1e-2)\n",
        "\n",
        "  def sub_forward(self, x):\n",
        "    x = self.conv_model(x)\n",
        "\n",
        "    x = x.view(x.size(0), -1)\n",
        "\n",
        "    x = torch.sigmoid(self.fc1(x))\n",
        "    return x\n",
        "\n",
        "  def forward(self, x1, x2):\n",
        "    x1 = self.sub_forward(x1)\n",
        "    x2 = self.sub_forward(x2)\n",
        "    diff = torch.abs(x1 - x2)\n",
        "\n",
        "    scores = self.fc2(diff)\n",
        "\n",
        "    return scores"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPV2bPC1h0Og",
        "outputId": "bb672d62-fb49-4f41-b15c-fe010b791eb0"
      },
      "source": [
        "best_valid_acc = 0.\n",
        "epochs = 200\n",
        "start_epoch = 0\n",
        "lr_patience = 1\n",
        "train_patience = 20\n",
        "counter = 0\n",
        "num_model = '1'\n",
        "ckpt_dir = './ckpt/'\n",
        "logs_dir = './logs/'\n",
        "plot_dir = './plots/'\n",
        "num_train = len(train)\n",
        "num_valid = len(val)\n",
        "ckpt_dir = os.path.join(ckpt_dir, num_model)\n",
        "logs_dir = os.path.join(logs_dir, num_model)\n",
        "os.makedirs(ckpt_dir)\n",
        "os.makedirs(logs_dir)\n",
        "layer_hyperparams = {\n",
        "            'layer_init_lrs': [],\n",
        "            'layer_end_momentums': [],\n",
        "            'layer_l2_regs': []\n",
        "        }\n",
        "print(\"[*] Sampling layer hyperparameters.\")\n",
        "\n",
        "layer_hyperparams = {\n",
        "    'layer_init_lrs': [],\n",
        "    'layer_end_momentums': [],\n",
        "    'layer_l2_regs': []\n",
        "}\n",
        "model = SiameseNetwork().to(device)\n",
        "num_layers = len(list(model.children()))\n",
        "init_momentum = 0.5\n",
        "for i in range(6):\n",
        "    # sample\n",
        "    lr = random.uniform(1e-4, 1e-1)\n",
        "    mom = random.uniform(0, 1)\n",
        "    reg = random.uniform(0, 0.1)\n",
        "\n",
        "    # store\n",
        "    layer_hyperparams['layer_init_lrs'].append(lr)\n",
        "    layer_hyperparams['layer_end_momentums'].append(mom)\n",
        "    layer_hyperparams['layer_l2_regs'].append(reg)\n",
        "        \n",
        "# grab layer-wise hyperparams\n",
        "init_lrs = layer_hyperparams['layer_init_lrs']\n",
        "init_momentums = [init_momentum]*num_layers\n",
        "end_momentums = layer_hyperparams['layer_end_momentums']\n",
        "l2_regs = layer_hyperparams['layer_l2_regs']\n",
        "\n",
        "if epochs == 1:\n",
        "    f = lambda max, min: min\n",
        "else:\n",
        "    f = lambda max, min: (max - min) / (epochs-1)\n",
        "momentum_temper_rates = [\n",
        "    f(x, y) for x, y in zip(end_momentums, init_momentums)\n",
        "]\n",
        "\n",
        "# lrs = init_lrs\n",
        "# momentums = init_momentums\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=3e-2, weight_decay=6e-5)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Sampling layer hyperparameters.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fruj5QYY_7ry"
      },
      "source": [
        "import shutil\n",
        "def save_checkpoint(state, is_best):\n",
        "  filename = 'model_ckpt.tar'\n",
        "  ckpt_path = os.path.join(ckpt_dir, filename)\n",
        "  torch.save(state, ckpt_path)\n",
        "\n",
        "  if is_best:\n",
        "    filename = 'best_model_ckpt.tar'\n",
        "    shutil.copyfile(ckpt_path, os.path.join(ckpt_dir, filename))\n",
        "\n",
        "def load_checkpoint(best=False, ckpt_dir = './ckpt/'):\n",
        "  print(\"[*] Loading model from {}\".format(ckpt_dir))\n",
        "\n",
        "  filename = 'model_ckpt.tar'\n",
        "  if best:\n",
        "      filename = 'best_model_ckpt.tar'\n",
        "  ckpt_path = os.path.join(ckpt_dir, filename)\n",
        "  ckpt = torch.load(ckpt_path)\n",
        "\n",
        "  # load variables from checkpoint\n",
        "  start_epoch = ckpt['epoch']\n",
        "  best_valid_acc = ckpt['best_valid_acc']\n",
        "  model.load_state_dict(ckpt['model_state'])\n",
        "  optimizer.load_state_dict(ckpt['optim_state'])\n",
        "\n",
        "  if best:\n",
        "      print(\n",
        "          \"[*] Loaded {} checkpoint @ epoch {} \"\n",
        "          \"with best valid acc of {:.3f}\".format(\n",
        "              filename, ckpt['epoch'], ckpt['best_valid_acc'])\n",
        "      )\n",
        "  else:\n",
        "      print(\n",
        "          \"[*] Loaded {} checkpoint @ epoch {}\".format(\n",
        "              filename, ckpt['epoch'])\n",
        "      )"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZ2eiMmCAxRl"
      },
      "source": [
        "# import tqdm\n",
        "        \n",
        "        "
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 979
        },
        "id": "vKUmvrIN7KE-",
        "outputId": "7ddae85a-e64e-46d3-d5c6-4ec62fa1811c"
      },
      "source": [
        "# if resume:\n",
        "#     self.load_checkpoint(best=False)\n",
        "\n",
        "# switch to train mode\n",
        "\n",
        "# create train and validation log files\n",
        "optim_file = open(os.path.join(logs_dir, 'optim.csv'), 'w')\n",
        "train_file = open(os.path.join(logs_dir, 'train.csv'), 'w')\n",
        "valid_file = open(os.path.join(logs_dir, 'valid.csv'), 'w')\n",
        "\n",
        "print(\"\\n[*] Train on {} sample pairs, validate on {} trials\".format(\n",
        "    num_train, num_valid)\n",
        ")\n",
        "\n",
        "for epoch in range(0, epochs):\n",
        "    \n",
        "    print('\\nEpoch: {}/{}'.format(epoch+1, epochs))\n",
        "    model.train()\n",
        "\n",
        "    with tqdm.tqdm(total=num_train) as pbar:\n",
        "      for i, sample in enumerate(train_loader):\n",
        "        # print(len(sample))\n",
        "        x1, x2, x3 = sample[\"main\"], sample[\"pos\"], sample[\"neg\"]\n",
        "        x1, x2, x3 = x1.to(device), x2.to(device), x3.to(device)\n",
        "        x1, x2, x3 = x1.unsqueeze(1), x2.unsqueeze(1), x3.unsqueeze(1)\n",
        "        # print(x1.shape)\n",
        "        # x1, x2, x3 = Variable(x1), Variable(x2), Variable(x3)\n",
        "\n",
        "        # split input pairs along the batch dimension\n",
        "        batch_size = x1.shape[0]\n",
        "\n",
        "        out1 = model(x1, x2)\n",
        "        out2 = model(x1, x3)\n",
        "        loss1 = F.binary_cross_entropy_with_logits(out1, torch.ones(out1.shape).to(device))\n",
        "        loss2 = F.binary_cross_entropy_with_logits(out2, torch.zeros(out2.shape).to(device))\n",
        "\n",
        "        # compute gradients and update\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss1 + loss2\n",
        "        loss.backward()\n",
        "        # loss2.backward()\n",
        "        optimizer.step()\n",
        "        print(loss1.item(), loss2.item())\n",
        "        # store batch statistics\n",
        "\n",
        "        pbar.set_description(\n",
        "            (\n",
        "                \"loss: {:.6f}\".format(\n",
        "                    loss.item(),\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "        pbar.update(batch_size)\n",
        "\n",
        "        # log loss\n",
        "        iter = (epoch * len(train_loader)) + i\n",
        "        train_file.write('{},{}\\n'.format(\n",
        "            iter, loss.item())\n",
        "        )\n",
        "\n",
        "    \n",
        "\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    for i, sample in enumerate(val_loader):\n",
        "        x1, x2, x3 = sample[\"main\"], sample[\"pos\"], sample[\"neg\"]\n",
        "        x1, x2, x3 = x1.to(device), x2.to(device), x3.to(device)\n",
        "        x1, x2, x3 = x1.unsqueeze(1), x2.unsqueeze(1), x3.unsqueeze(1)\n",
        "\n",
        "        # x1, x2 = Variable(x1, volatile=True), Variable(x2, volatile=True)\n",
        "\n",
        "        batch_size = x1.shape[0]\n",
        "\n",
        "        # compute log probabilities\n",
        "        out1 = model(x1, x2)\n",
        "        out2 = model(x1, x3)\n",
        "        log_probas1 = (torch.sigmoid(out1)>0.5)\n",
        "        log_probas2 = (torch.sigmoid(out2) < 0.5)\n",
        "        \n",
        "        for t in log_probas1:\n",
        "          if t.item():\n",
        "            correct+=1\n",
        "        for t in log_probas2:\n",
        "          if t.item():\n",
        "            correct+=1\n",
        "\n",
        "        # get index of max log prob\n",
        "        # print(log_probas)\n",
        "        # pred = log_probas.data.max(0)[1][0]\n",
        "\n",
        "        # if pred == 0:\n",
        "            # correct += 1\n",
        "\n",
        "    # compute acc and log\n",
        "    valid_acc = (100. * correct) / (2*num_valid)\n",
        "    iter = epoch\n",
        "    valid_file.write('{},{}\\n'.format(\n",
        "        iter, valid_acc)\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # check for improvement\n",
        "    is_best = valid_acc > best_valid_acc\n",
        "    msg = \"val acc: {:.6f}\"\n",
        "    if is_best:\n",
        "        msg += \" [*]\"\n",
        "        counter = 0\n",
        "    print(msg.format(valid_acc))\n",
        "\n",
        "    # checkpoint the model\n",
        "    if not is_best:\n",
        "        counter += 1\n",
        "    if counter > train_patience:\n",
        "        print(\"[!] No improvement in a while, stopping training.\")\n",
        "        break\n",
        "    best_valid_acc = max(valid_acc, best_valid_acc)\n",
        "    save_checkpoint(\n",
        "        {\n",
        "            'epoch': epoch + 1,\n",
        "            'model_state': model.state_dict(),\n",
        "            'optim_state': optimizer.state_dict(),\n",
        "            'best_valid_acc': best_valid_acc,\n",
        "        }, is_best\n",
        "    )\n",
        "# release resources\n",
        "optim_file.close()\n",
        "train_file.close()\n",
        "valid_file.close()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1216 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[*] Train on 1216 sample pairs, validate on 304 trials\n",
            "\n",
            "Epoch: 1/200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 1.304819:   3%|▎         | 40/1216 [00:14<06:51,  2.86it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.6956269145011902 0.6091917157173157\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 1.599971:   7%|▋         | 80/1216 [00:29<06:46,  2.79it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.6807022094726562 0.9192692041397095\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 1.133388:  10%|▉         | 120/1216 [00:43<06:36,  2.77it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.6852103471755981 0.44817790389060974\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 1.357894:  13%|█▎        | 160/1216 [00:58<06:22,  2.76it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.6790521740913391 0.6788418292999268\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 1.226510:  16%|█▋        | 200/1216 [01:12<06:05,  2.78it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.6723537445068359 0.5541563034057617\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 1.318725:  20%|█▉        | 240/1216 [01:27<05:52,  2.77it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.6641027927398682 0.6546218395233154\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 1.340106:  23%|██▎       | 280/1216 [01:43<05:52,  2.66it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.656445324420929 0.6836608052253723\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 1.148430:  26%|██▋       | 320/1216 [01:59<05:44,  2.60it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.650002658367157 0.4984269142150879\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 1.071520:  30%|██▉       | 360/1216 [02:13<05:19,  2.68it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.6417508125305176 0.4297691881656647\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 0.958161:  33%|███▎      | 400/1216 [02:28<05:03,  2.69it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.6319214701652527 0.3262399435043335\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 0.977423:  36%|███▌      | 440/1216 [02:40<04:31,  2.85it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.6209256649017334 0.35649725794792175\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 0.938475:  39%|███▉      | 480/1216 [02:54<04:19,  2.84it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.6092085838317871 0.32926657795906067\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 0.817904:  43%|████▎     | 520/1216 [03:07<03:59,  2.91it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.5969721078872681 0.22093148529529572\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 0.858362:  46%|████▌     | 560/1216 [03:22<03:48,  2.87it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.5843448638916016 0.27401724457740784\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 0.919798:  49%|████▉     | 600/1216 [03:34<03:28,  2.95it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.5715107321739197 0.34828680753707886\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 0.765930:  53%|█████▎    | 640/1216 [03:48<03:15,  2.95it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.5587897300720215 0.20713987946510315\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 0.669168:  56%|█████▌    | 680/1216 [04:01<03:00,  2.98it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.5458890199661255 0.12327849864959717\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 0.625860:  59%|█████▉    | 720/1216 [04:13<02:42,  3.06it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.5328451991081238 0.09301453083753586\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 0.619883:  62%|██████▎   | 760/1216 [04:26<02:26,  3.11it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.5197211503982544 0.1001618281006813\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 0.588331:  66%|██████▌   | 800/1216 [04:38<02:12,  3.14it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.5065948963165283 0.08173643797636032\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 0.600833:  69%|██████▉   | 840/1216 [04:51<01:59,  3.15it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.49351391196250916 0.10731891542673111\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 0.594351:  72%|███████▏  | 880/1216 [05:03<01:45,  3.19it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.4805756211280823 0.11377567052841187\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 0.527477:  76%|███████▌  | 920/1216 [05:16<01:33,  3.17it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.4678347706794739 0.059642672538757324\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 0.496958:  79%|███████▉  | 960/1216 [05:28<01:21,  3.16it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.4552314281463623 0.041726596653461456\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 0.478963:  82%|████████▏ | 1000/1216 [05:41<01:08,  3.14it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.44278189539909363 0.036181267350912094\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 0.446386:  86%|████████▌ | 1040/1216 [05:55<00:57,  3.04it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.43051645159721375 0.015869256108999252\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 0.459503:  89%|████████▉ | 1080/1216 [06:10<00:45,  2.97it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.4184378683567047 0.04106496274471283\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 0.488802:  92%|█████████▏| 1120/1216 [06:23<00:32,  3.00it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.40662652254104614 0.08217574656009674\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 0.433938:  95%|█████████▌| 1160/1216 [06:35<00:18,  3.04it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.39518365263938904 0.03875439986586571\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 0.429301:  99%|█████████▊| 1200/1216 [06:49<00:05,  3.01it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.38402196764945984 0.04527931287884712\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 0.379951: 100%|██████████| 1216/1216 [06:54<00:00,  2.93it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.3731657862663269 0.006785426288843155\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "val acc: 99.835526 [*]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-3ca8bddbf765>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0;34m'optim_state'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;34m'best_valid_acc'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbest_valid_acc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         }, is_best\n\u001b[0m\u001b[1;32m    126\u001b[0m     )\n\u001b[1;32m    127\u001b[0m \u001b[0;31m# release resources\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-db12448b29ad>\u001b[0m in \u001b[0;36msave_checkpoint\u001b[0;34m(state, is_best)\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mis_best\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'best_model_ckpt.tar'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./ckpt/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'shutil' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "708OeXKsF9Li"
      },
      "source": [
        "torch.ones((1,1))\n",
        "model = SiameseNetwork().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzW4Kr8hPlTp"
      },
      "source": [
        "for sample in train_loader:\n",
        "  x1, x2, x3 = sample[\"main\"], sample[\"pos\"], sample[\"neg\"]\n",
        "  x1, x2, x3 = x1.to(device), x2.to(device), x3.to(device)\n",
        "  x1, x2, x3 = x1.unsqueeze(1), x2.unsqueeze(1), x3.unsqueeze(1)\n",
        "  out1 = model(x1, x2)\n",
        "  out2 = model(x1, x3)\n",
        "  log_probas1 = (torch.sigmoid(out1))\n",
        "  log_probas2 = (torch.sigmoid(out2))\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWcxxaXh2FI9"
      },
      "source": [
        "log_probas1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3FSoazM3gKs",
        "outputId": "239c995e-104a-4592-ebe8-3161890e518c"
      },
      "source": [
        "for t in torch.sigmoid(out2):\n",
        "  print(t.item())"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.013550231233239174\n",
            "0.27653956413269043\n",
            "0.011210781522095203\n",
            "0.004630005918443203\n",
            "0.0009928708896040916\n",
            "0.008655696175992489\n",
            "0.005865841638296843\n",
            "0.00010199644020758569\n",
            "0.06163925677537918\n",
            "0.001496527111157775\n",
            "0.00010099974315380678\n",
            "9.675189858171507e-07\n",
            "0.030920496210455894\n",
            "0.20706304907798767\n",
            "0.004100235179066658\n",
            "0.0011194780236110091\n",
            "0.012206608429551125\n",
            "0.04572686180472374\n",
            "0.004605553112924099\n",
            "0.0020429904107004404\n",
            "0.0064968205988407135\n",
            "0.005293260794132948\n",
            "1.0012881830334663e-05\n",
            "4.8767476982902735e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6bfec2o4pZr"
      },
      "source": [
        "transformed_test_dataset = GetData(test_set, transform=transforms.Compose([Resize(), ToTensor()]))\n",
        "test_loader = DataLoader(transformed_test_dataset, batch_size=40, shuffle=True)\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsbCRoV29hss"
      },
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "for i, sample in enumerate(test_loader):\n",
        "    x1, x2, x3 = sample[\"main\"], sample[\"pos\"], sample[\"neg\"]\n",
        "    x1, x2, x3 = x1.to(device), x2.to(device), x3.to(device)\n",
        "    x1, x2, x3 = x1.unsqueeze(1), x2.unsqueeze(1), x3.unsqueeze(1)\n",
        "\n",
        "    # x1, x2 = Variable(x1, volatile=True), Variable(x2, volatile=True)\n",
        "\n",
        "    batch_size = x1.shape[0]\n",
        "\n",
        "    # compute log probabilities\n",
        "    out1 = model(x1, x2)\n",
        "    out2 = model(x1, x3)\n",
        "    log_probas1 = (torch.sigmoid(out1)>0.5)\n",
        "    log_probas2 = (torch.sigmoid(out2) < 0.5)\n",
        "    \n",
        "    for t in log_probas1:\n",
        "      if t.item():\n",
        "        correct+=1\n",
        "    for t in log_probas2:\n",
        "      if t.item():\n",
        "        correct+=1\n",
        "\n",
        "    # get index of max log prob\n",
        "    # print(log_probas)\n",
        "    # pred = log_probas.data.max(0)[1][0]\n",
        "\n",
        "    # if pred == 0:\n",
        "        # correct += 1\n",
        "\n",
        "# compute acc and log\n",
        "test_acc = (100. * correct) / (2*num_valid)\n",
        "iter = epoch\n",
        "# test_file.write('{},{}\\n'.format(\n",
        "#     iter, valid_acc)\n",
        "# )"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DArmbFgc-DPb",
        "outputId": "05f6143b-381d-49ce-a8f6-22c3c5864037"
      },
      "source": [
        "test_acc * (num_valid)/(len(test_loader)*40)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNVLcxJN_ZAZ",
        "outputId": "d478d908-1f40-44c0-d52d-4743a9ad998c"
      },
      "source": [
        "len(test_loader)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7Nt6ZM0_1Di"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}